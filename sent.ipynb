{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "def remove_punctuation(x):\n",
    "    \"\"\"\n",
    "    Helper function to remove punctuation from a string\n",
    "    x: any string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = ''.join(ch for ch in x if ch not in exclude)\n",
    "    except:\n",
    "        pass\n",
    "    return x\n",
    "# Apply the function to the DataFrame\n",
    "df.review = df.review.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'],\n",
    "                                                    df['target'],\n",
    "                                                    train_size = 0.8,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=14\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train first entry:\n\n I felt that the movie was dry very disappointing no plotkept waited for something to happened and nothing did dry as a bone a wast of money One of Robins Williams worst filmsif you dont believe me wait a few months it will be out on DVD because that seems to be a pattern for movies that dont do well in the theaters are out as rentals before the year is over This is one you will not want to see or say why did I spend my money on that Plus for it being such a new movie there were only 8 people in the watching it This was on a Friday night the 950 showing I also felt that it needed some more excitement or something to keep us awake When they characters spoke in the movie the voices were also very low you could not hear what they were saying\n\n\nX_train shape:  (16000,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train first entry:\\n\\n', X_train[0])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train first entry:\n\n I've read all the rave reviews here and am impressed with the imagination of those who loved this film. I can't say that I found much to recommend it. The Leonard Cohen sound track is not only excessively heavy-handed but dreary beyond measure. The film looks authentic enough, but something's got to happen for it to work, and nothing much does: a cursory plot (not a real problem for me), not much character development, nothing thematically. It just slogs along. Flawed as it is, Cimino's Heaven's Gate\" has some moments of genuine wonder and is a film I'd sooner watch again. For a brilliant reconception of the West, HBO's \"Deadwood\" is much superior to \"McCabe.\"\"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-88b3f07323c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train_NoStopNoPunct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train_NoStop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_train first entry:\\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_NoStopNoPunct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\nX_train shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_NoStopNoPunct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer        # “Term Frequency times Inverse Document Frequency”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vect = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n 'aaa',\n 'ailment',\n 'antired',\n 'authoress',\n 'beefed',\n 'blowed',\n 'btastwoface',\n 'casing',\n 'churns',\n 'compile',\n 'craft',\n 'ddvd',\n 'diered',\n 'doubletime',\n 'eitherand',\n 'etcrobert',\n 'fanin',\n 'fittest',\n 'frumpiness',\n 'gleanne',\n 'guttedthe',\n 'helpthen',\n 'host',\n 'inconvenient',\n 'italy',\n 'kassar',\n 'latched',\n 'lizziesome',\n 'mallorquins',\n 'melendez',\n 'mommys',\n 'muttering',\n 'nonthreatening',\n 'oneway',\n 'panoply',\n 'physicists',\n 'powcamp',\n 'purists',\n 'rediscover',\n 'riding',\n 'sandersjealous',\n 'selectively',\n 'shrek',\n 'softcentred',\n 'starters',\n 'suicidally',\n 'tears',\n 'thuggee',\n 'treesflavia',\n 'unfathomable',\n 'videobox',\n 'well',\n 'worldsolo']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[::1999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107662"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16000x62715 sparse matrix of type '<class 'numpy.int64'>'\n\twith 2185830 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.879988079809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score  \n",
    "# \n",
    "# predictions = model.predict(vect.transform(X_test))\n",
    "# print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n['waste' 'worst' 'boring' 'disappointment' 'awful' 'mess' 'lacks'\n 'forgettable' 'poorly' 'britney']\n\nLargest Coefs:\n['excellent' 'superb' 'loved' 'amazing' 'rare' 'perfect' 'favorite'\n 'touching' 'australia' 'flight']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs:\\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21803"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.884315901579\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.884315901579\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n['cruiserweight' 'suplexes' 'pup' 'annihilated' 'gauche' 'statistic'\n 'booed' 'hypocrites' 'mimics' 'oncoming']\n\nLargest tfidf:\n['name' 'pokemon' 'steve' 'dev' 'smallville' 'wei' 'woo' 'doodlebops'\n 'casper' 'weller']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf:\\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n['worst' 'bad' 'awful' 'boring' 'waste' 'poor' 'nothing' 'terrible' 'no'\n 'worse']\n\nLargest Coefs:\n['great' 'excellent' 'best' 'perfect' 'wonderful' 'well' 'amazing' 'loved'\n 'love' 'favorite']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs:\\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128769"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.896329193644\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n['worst' 'awful' 'boring' 'waste' 'disappointment' 'poor' 'poorly'\n 'disappointing' 'the worst' 'lame']\n\nLargest Coefs:\n['excellent' 'perfect' 'wonderful' 'superb' 'amazing' 'enjoyable'\n 'brilliant' 'well worth' 'rare' 'refreshing']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs:\\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The END\n"
     ]
    }
   ],
   "source": [
    "print('The END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201791"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5,\n",
    "                       ngram_range=(1,3),\n",
    "                       # stop_words='english'\n",
    "                       ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.898928105737\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()                        # With 75% data\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n['worst' 'awful' 'boring' 'waste' 'disappointing' 'poor' 'poorly'\n 'terrible' 'lacks' 'disappointment']\n\nLargest Coefs:\n['excellent' 'wonderful' 'perfect' 'great' 'superb' 'brilliant' 'enjoyable'\n 'amazing' 'enjoyed' 'must see']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs:\\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.898117746903\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()            # 80% data, 1-3grams, min_df=5\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('Accuracy: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201935"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5,\n",
    "                       ngram_range=(1,3),       # now without stopwords\n",
    "                       # stop_words='english'\n",
    "                       ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.898050876385\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()            # 80% data, 1-3grams, min_df=5, w/o stopwords\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('Accuracy: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19949"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer = 'char',\n",
    "                       min_df=5,\n",
    "                       ngram_range=(1,3),       \n",
    "                       # stop_words='english'\n",
    "                       ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.834418669109\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()            \n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('Accuracy: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b345bc318518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\"txt_sentoken/neg/cv000_29416.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'r' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print(model.predict(vect.transform(\"txt_sentoken/neg/cv000_29416.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n['dul' 'ntm' 'hac' '4/1' 'lri' ' ok' 'x i' 'b m' 'oor' 'd!!']\n\nLargest Coefs:\n['7/' '7/1' 'rld' 't 5' 'm..' ' 8' 'awe' 'a 9' 'ct.' 's.b']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs:\\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5,\n",
    "                       ngram_range=(1,3),       # now without stopwords\n",
    "                       # stop_words='english'\n",
    "                       ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89419999999999999"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train_vectorized, y_train)\n",
    "clf.score(vect.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm                             \n",
    "clf = svm.SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "clf.fit(X_train_vectorized, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0038167925652689005"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_vectorized, y_test)    # accuracy of the above SVR classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = pd.read_csv('test_data.csv')     # reading in the testing data file\n",
    "#   in order to start cross validating on the full test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(test_doc['review'],\n",
    "                                                    test_doc['target'],\n",
    "                                                    \n",
    "                                                    test_size = .999999,\n",
    "                                                    random_state=14\n",
    "                                                    )\n",
    "\n",
    "# This is the actual testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88424999999999998"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC       # the new Linear SVC model. Works FAST!!\n",
    "clf = LinearSVC(random_state=0)         # we should work on it some.\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "clf.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86783042  0.85037406  0.8875      0.83        0.85        0.855       0.8325\n  0.85        0.83458647  0.8320802 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score     # Cross validation on just the 4 thousand reviews.\n",
    "scores = cross_val_score(clf, X_test_vectorized, y_test, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.854  0.86   0.862  0.834  0.858  0.864  0.842  0.856  0.852  0.82 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score # Cross validation on test data\n",
    "scores = cross_val_score(clf, vect.transform(X_test2), y_test2, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213794"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=4,          # first run with 4\n",
    "                       ngram_range=(1,3),       # now without stopwords\n",
    "                       # stop_words='english'\n",
    "                       ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88775000000000004"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC       # the new Linear SVC model\n",
    "clf = LinearSVC(random_state=0)         # we should work on it some\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "clf.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88549999999999995"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC       # the new Linear SVC model\n",
    "clf = LinearSVC(C=2)         # we should work on it some\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "clf.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88575000000000004"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC       # the new Linear SVC model\n",
    "clf = LinearSVC(C=1.2)         # we should work on it some\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "clf.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC       # the SVC model\n",
    "clf = SVC()         # \n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "clf.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205559"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=4,          # now run with 4 min_df and no punct\n",
    "                       ngram_range=(1,3),       # now without stopwords\n",
    "                       # stop_words='english'\n",
    "                       ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87749999999999995"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC       # Take 1 with no puncuation\n",
    "clf = LinearSVC()         # we should work on it some\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "clf.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n['name' 'worst' 'boring' 'awful' 'waste' 'terrible' 'poor' 'disappointment'\n 'mess' 'dull']\n\nLargest Coefs:\n['excellent' 'amazing' 'superb' 'wonderful' '710' 'touching' 'the name'\n 'perfect' 'today' 'very good']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = clf.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs:\\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87749999999999995"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear SVC Model\n",
    "from sklearn.svm import LinearSVC       # Take 2 with no puncuation\n",
    "clf = LinearSVC().fit(X_train_vectorized, y_train)\n",
    "clf.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR_model = LogisticRegression().fit(X_train_vectorized, y_train)\n",
    "clf.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mulitnomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB       # Multinomial Naive Bayes, supposedly goes well with the data from the transformer\n",
    "clf = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "clf.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes model\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB().fit(X_train_vectorized, y_train)\n",
    "clf.score(X_test_vectorized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB().fit((X_train_vectorized.toarray()), y_train)\n",
    "clf.score((X_test_vectorized.toarray()), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "# df.head()             # shows the first part of the file\n",
    "# In our original code, we imported, then Vectorized, then split, then train the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'],\n",
    "                                                    df['target'],\n",
    "                                                    random_state=0)\n",
    "print('X_train first entry:\\n\\n', X_train[0])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "X_train_vectorized = CountVectorizer(df_min = 5,            # Minimum document frequency to become a feature\n",
    "                                     ngram_range= (1,3),\n",
    "\n",
    "\n",
    ").fit_transform(X_train)\n",
    "# vect = CountVectorizer().fit(X_train)\n",
    "# X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "# vect.get_feature_names()[::2000]\n",
    "# len(vect.get_feature_names())\n",
    "# X_train_vectorized\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs:\\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data into a working variable...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Data into subsets...\nTraining shape is:  (16000,)\nExtracting features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719850 features found.\nTransforming for tfidf...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Evaluating Bernoulli Naive Bayes model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB accuracy, on validation data:  0.894\nTraining/Evaluating Multinomial Naive Bayes model...\nMultinomialNB accuracy, on validation data:  0.89225\nTraining/Evaluating Linear SVC model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC accuracy, on validation data:  0.90975\nTraining/Evaluating Logistic Regression model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy, on validation data:  0.8965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nLinearSVC Model statistics, on Test data (with tfidf)\nAccuracy: 0.9044\nF1 score: 0.904386231617\nRecall: 0.9044\nPrecision: 0.904633068648\n\n clasification report:\n              precision    recall  f1-score   support\n\n          0       0.91      0.89      0.90      2500\n          1       0.89      0.92      0.91      2500\n\navg / total       0.90      0.90      0.90      5000\n\n\n confussion matrix:\n [[2231  269]\n [ 209 2291]]\n\nLogistic Regression Model statistics, on Test data (with tfidf)\nAccuracy: 0.8846\nF1 score: 0.883981738685\nRecall: 0.8846\nPrecision: 0.892976691149\n\n clasification report:\n              precision    recall  f1-score   support\n\n          0       0.84      0.96      0.89      2500\n          1       0.95      0.81      0.88      2500\n\navg / total       0.89      0.88      0.88      5000\n\n\n confussion matrix:\n [[2394  106]\n [ 471 2029]]\n\nMultinomial Naive Bayes Model statistics, on Test data (with tfidf)\nAccuracy: 0.8908\nF1 score: 0.890789078908\nRecall: 0.8908\nPrecision: 0.890956382553\n\n clasification report:\n              precision    recall  f1-score   support\n\n          0       0.90      0.88      0.89      2500\n          1       0.88      0.90      0.89      2500\n\navg / total       0.89      0.89      0.89      5000\n\n\n confussion matrix:\n [[2202  298]\n [ 248 2252]]\n\nBernoulli Naive Bayes Model statistics, on Test data (with tfidf)\nAccuracy: 0.8898\nF1 score: 0.889798726073\nRecall: 0.8898\nPrecision: 0.889818025185\n\n clasification report:\n              precision    recall  f1-score   support\n\n          0       0.89      0.89      0.89      2500\n          1       0.89      0.89      0.89      2500\n\navg / total       0.89      0.89      0.89      5000\n\n\n confussion matrix:\n [[2216  284]\n [ 267 2233]]\ntime to run:  98.97754979133606\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer   # “Term Frequency times Inverse Document Frequency”\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB       # Multinomial Naive Bayes, supposedly goes well with the data from the transformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Reading the data into a working variable...\")        # READ DATA IN\n",
    "df = pd.read_csv('train_data.csv')      # to read the training data into working memory\n",
    "\n",
    "# DATA SPLITING\n",
    "print(\"Splitting Data into subsets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'],   # Test-Train Split function\n",
    "                                                    df['target'],\n",
    "                                                    train_size = 0.8,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=14\n",
    "                                                    )\n",
    "print(\"Training shape is: \", X_train.shape)\n",
    "\n",
    "# FEATURE EXTRACTION\n",
    "print(\"Extracting features...\")\n",
    "vect = CountVectorizer(min_df=2,          # Minimum Document Frequency\n",
    "                       ngram_range=(1,4),   # unigrams to trigrams\n",
    "                       # stop_words='english'   # stopwords\n",
    "                       ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "print(len(vect.get_feature_names()), \"features found.\")     # Feature check\n",
    "\n",
    "# TFIDF Transformer\n",
    "print(\"Transforming for tfidf...\")\n",
    "X_train_vectorized_tfidf = TfidfTransformer().fit_transform(X_train_vectorized)     # or X_train_dense\n",
    "X_test_vectorized_tfidf = TfidfTransformer().fit_transform(X_test_vectorized)       # or X_test_dense\n",
    "\n",
    "# # Numpy Dense Array Transformation\n",
    "# print(\"Converting to Numpy Dense Array...\")\n",
    "# X_train_dense = X_train_vectorized.toarray()   # Convert to Dense Numpy Array\n",
    "# X_test_dense = X_test_vectorized.toarray()\n",
    "\n",
    "# # Classifier Comparison\n",
    "\n",
    "# # Gaussian Naive Bayes model\n",
    "# print(\"Training/Evaluating Gaussian Naive Bayes model...\")\n",
    "# gnb_model = GaussianNB().fit((X_train_dense), y_train)\n",
    "# print(\"GaussianNB accuracy, on validation data: \", gnb_model.score((X_test_dense), y_test))\n",
    "\n",
    "\n",
    "# Bernoulli Naive Bayes model\n",
    "print(\"Training/Evaluating Bernoulli Naive Bayes model...\")\n",
    "bnb_model = BernoulliNB().fit(X_train_vectorized, y_train)\n",
    "print(\"BernoulliNB accuracy, on validation data: \", bnb_model.score(X_test_vectorized, y_test))\n",
    "\n",
    "\n",
    "# Mulitnomial Naive Bayes model\n",
    "print(\"Training/Evaluating Multinomial Naive Bayes model...\")\n",
    "mnb_model = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "print(\"MultinomialNB accuracy, on validation data: \", mnb_model.score(X_test_vectorized, y_test))\n",
    "\n",
    "\n",
    "# Linear SVC Model\n",
    "print(\"Training/Evaluating Linear SVC model...\")\n",
    "lsvc_model = LinearSVC().fit(X_train_vectorized_tfidf, y_train)\n",
    "print(\"Linear SVC accuracy, on validation data: \", lsvc_model.score(X_test_vectorized_tfidf, y_test))\n",
    "\n",
    "\n",
    "# Logistic Regression Model\n",
    "print(\"Training/Evaluating Logistic Regression model...\")\n",
    "lr_model = LogisticRegression().fit(X_train_vectorized, y_train)\n",
    "print(\"Logistic Regression accuracy, on validation data: \", lr_model.score(X_test_vectorized, y_test))\n",
    "\n",
    "\n",
    "# # svm.SVC model\n",
    "# clfrSVM = svm.SVC(kernel='linear', C=0.1)             # SVM Classifier training without tfidf\n",
    "# clfrSVM.fit(X_train_vectorized, y_train)\n",
    "# predicted_labels = clfrSVM.predict(X_test_vectorized)\n",
    "# print(\"SVM, Accuracy on validation data:\", accuracy_score(y_test, predicted_labels))\n",
    "#\n",
    "# clfrSVMtfidf = svm.SVC(kernel='linear', C=0.1)          # SVM Classifier training with tfidf\n",
    "# clfrSVMtfidf.fit(X_train_vectorized_tfidf, y_train)\n",
    "# predicted_labels_tfidf = clfrSVMtfidf.predict(X_test_vectorized_tfidf)\n",
    "# print(\"SVM, Accuracy on validation data with tfidf:\", accuracy_score(y_test, predicted_labels_tfidf))\n",
    "\n",
    "\n",
    "# Final statistics\n",
    "\n",
    "          # This is the actual testing data being read into a new variable\n",
    "test_doc = pd.read_csv('test_data.csv')     # reading in the testing data file for final testing\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(test_doc['review'], # creating a similar variable with test data\n",
    "                                                        test_doc['target'],\n",
    "                                                        test_size=.999999,\n",
    "                                                        random_state=0\n",
    "                                                        )\n",
    "\n",
    "X_test2_vect = vect.transform(X_test2)          # Test Data feature extraction\n",
    "# print('\\n\\nX_train shape: ', X_test2_vect.shape)      # Are we on the right track?\n",
    "X_test2_final = TfidfTransformer().fit_transform(X_test2_vect)      # Tfidf transformation of Test Data\n",
    "# X_testFinalDense = X_test2_final.toarray()        # Numpy vector smoothing\n",
    "\n",
    "# Elicitation of LinearSVC Stats\n",
    "weighted_prediction = lsvc_model.predict(X_test2_final)\n",
    "print(\"\\nLinearSVC Model statistics, on Test data (with tfidf)\")    # statistics for the LinearSVC model with tfidf\n",
    "print('Accuracy:', accuracy_score(y_test2, weighted_prediction))\n",
    "print('F1 score:', f1_score(y_test2, weighted_prediction,average='weighted'))\n",
    "print('Recall:', recall_score(y_test2, weighted_prediction,\n",
    "                              average='weighted'))\n",
    "print('Precision:', precision_score(y_test2, weighted_prediction,\n",
    "                                    average='weighted'))\n",
    "print('\\n clasification report:\\n', classification_report(y_test2, weighted_prediction))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(y_test2, weighted_prediction))\n",
    "\n",
    "# Elicitation of Logistic Regression Stats\n",
    "lrweighted_prediction = lr_model.predict(X_test2_final)\n",
    "print(\"\\nLogistic Regression Model statistics, on Test data (with tfidf)\")    # statistics for the LogisticRegression model with tfidf\n",
    "print('Accuracy:', accuracy_score(y_test2, lrweighted_prediction))\n",
    "print('F1 score:', f1_score(y_test2, lrweighted_prediction,average='weighted'))\n",
    "print('Recall:', recall_score(y_test2, lrweighted_prediction,\n",
    "                              average='weighted'))\n",
    "print('Precision:', precision_score(y_test2, lrweighted_prediction,\n",
    "                                    average='weighted'))\n",
    "print('\\n clasification report:\\n', classification_report(y_test2, lrweighted_prediction))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(y_test2, lrweighted_prediction))\n",
    "\n",
    "# Elicitation of Multinomial Naive Bayes Stats\n",
    "mnbweighted_prediction = mnb_model.predict(X_test2_final)\n",
    "print(\"\\nMultinomial Naive Bayes Model statistics, on Test data (with tfidf)\")    # statistics for the MultinomialNB model with tfidf\n",
    "print('Accuracy:', accuracy_score(y_test2, mnbweighted_prediction))\n",
    "print('F1 score:', f1_score(y_test2, mnbweighted_prediction,average='weighted'))\n",
    "print('Recall:', recall_score(y_test2, mnbweighted_prediction,\n",
    "                              average='weighted'))\n",
    "print('Precision:', precision_score(y_test2, mnbweighted_prediction,\n",
    "                                    average='weighted'))\n",
    "print('\\n clasification report:\\n', classification_report(y_test2, mnbweighted_prediction))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(y_test2, mnbweighted_prediction))\n",
    "\n",
    "# Elicitation of Bernoulli Naive Bayes Stats\n",
    "bnbweighted_prediction = bnb_model.predict(X_test2_final)\n",
    "print(\"\\nBernoulli Naive Bayes Model statistics, on Test data (with tfidf)\")    # statistics for the BernoulliNB model with tfidf\n",
    "print('Accuracy:', accuracy_score(y_test2, bnbweighted_prediction))\n",
    "print('F1 score:', f1_score(y_test2, bnbweighted_prediction,average='weighted'))\n",
    "print('Recall:', recall_score(y_test2, bnbweighted_prediction,\n",
    "                              average='weighted'))\n",
    "print('Precision:', precision_score(y_test2, bnbweighted_prediction,\n",
    "                                    average='weighted'))\n",
    "print('\\n clasification report:\\n', classification_report(y_test2, bnbweighted_prediction))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(y_test2, bnbweighted_prediction))\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"time to run: \", t1-t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n['bad' 'worst' 'boring' 'awful' 'poor' 'the worst' 'waste' 'no' 'terrible'\n 'nothing' 'stupid' 'worse' 'unfortunately' 'horrible' 'poorly' 'dull'\n 'script' 'lame' 'fails' 'annoying']\n\nLargest Coefs:\n['great' 'excellent' 'amazing' 'wonderful' 'perfect' 'fun' 'loved' 'today'\n 'the best' 'enjoyed' 'love' 'bit' 'superb' 'well' 'still' 'best'\n 'definitely' 'brilliant' 'it' 'life']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = lsvc_model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:20]]))\n",
    "print('Largest Coefs:\\n{}'.format(feature_names[sorted_coef_index[:-21:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data into a working variable...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Data into subsets...\nTraining shape is:  (16000,)\nExtracting features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682769 features found.\nTransforming for tfidf...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Evaluating Bernoulli Naive Bayes model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB accuracy, on validation data:  0.8895\nTraining/Evaluating Multinomial Naive Bayes model...\nMultinomialNB accuracy, on validation data:  0.8915\nTraining/Evaluating Linear SVC model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC accuracy, on validation data:  0.892\nTraining/Evaluating Logistic Regression model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy, on validation data:  0.87525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nLinearSVC Model statistics, on Test data (with tfidf)\nAccuracy: 0.8856\nF1 score: 0.885584604264\nRecall: 0.8856\nPrecision: 0.885807657113\n\n clasification report:\n              precision    recall  f1-score   support\n\n          0       0.89      0.87      0.88      2500\n          1       0.88      0.90      0.89      2500\n\navg / total       0.89      0.89      0.89      5000\n\n\n confussion matrix:\n [[2185  315]\n [ 257 2243]]\n\nLogistic Regression Model statistics, on Test data (with tfidf)\nAccuracy: 0.8832\nF1 score: 0.883199084281\nRecall: 0.8832\nPrecision: 0.883212017529\n\n clasification report:\n              precision    recall  f1-score   support\n\n          0       0.89      0.88      0.88      2500\n          1       0.88      0.89      0.88      2500\n\navg / total       0.88      0.88      0.88      5000\n\n\n confussion matrix:\n [[2201  299]\n [ 285 2215]]\n\nMultinomial Naive Bayes Model statistics, on Test data (with tfidf)\nAccuracy: 0.8842\nF1 score: 0.884177942847\nRecall: 0.8842\nPrecision: 0.884492891305\n\n clasification report:\n              precision    recall  f1-score   support\n\n          0       0.90      0.87      0.88      2500\n          1       0.87      0.90      0.89      2500\n\navg / total       0.88      0.88      0.88      5000\n\n\n confussion matrix:\n [[2176  324]\n [ 255 2245]]\n\nBernoulli Naive Bayes Model statistics, on Test data (with tfidf)\nAccuracy: 0.8824\nF1 score: 0.882387279008\nRecall: 0.8824\nPrecision: 0.882565513144\n\n clasification report:\n              precision    recall  f1-score   support\n\n          0       0.89      0.87      0.88      2500\n          1       0.87      0.89      0.88      2500\n\navg / total       0.88      0.88      0.88      5000\n\n\n confussion matrix:\n [[2180  320]\n [ 268 2232]]\ntime to run:  84.12807703018188\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer   # “Term Frequency times Inverse Document Frequency”\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB       # Multinomial Naive Bayes, supposedly goes well with the data from the transformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Reading the data into a working variable...\")        # READ DATA IN\n",
    "df = pd.read_csv('train_data.csv')      # to read the training data into working memory\n",
    "\n",
    "# DATA SPLITING\n",
    "print(\"Splitting Data into subsets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'],   # Test-Train Split function\n",
    "                                                    df['target'],\n",
    "                                                    train_size = 0.8,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=14\n",
    "                                                    )\n",
    "print(\"Training shape is: \", X_train.shape)\n",
    "\n",
    "# FEATURE EXTRACTION\n",
    "print(\"Extracting features...\")\n",
    "vect = CountVectorizer(min_df=2,          # Minimum Document Frequency\n",
    "                       ngram_range=(2,4),   # unigrams to trigrams\n",
    "                       # stop_words='english'   # stopwords\n",
    "                       ).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "print(len(vect.get_feature_names()), \"features found.\")     # Feature check\n",
    "\n",
    "# TFIDF Transformer\n",
    "print(\"Transforming for tfidf...\")\n",
    "X_train_vectorized_tfidf = TfidfTransformer().fit_transform(X_train_vectorized)     # or X_train_dense\n",
    "X_test_vectorized_tfidf = TfidfTransformer().fit_transform(X_test_vectorized)       # or X_test_dense\n",
    "\n",
    "# # Numpy Dense Array Transformation\n",
    "# print(\"Converting to Numpy Dense Array...\")\n",
    "# X_train_dense = X_train_vectorized.toarray()   # Convert to Dense Numpy Array\n",
    "# X_test_dense = X_test_vectorized.toarray()\n",
    "\n",
    "# # Classifier Comparison\n",
    "\n",
    "# # Gaussian Naive Bayes model\n",
    "# print(\"Training/Evaluating Gaussian Naive Bayes model...\")\n",
    "# gnb_model = GaussianNB().fit((X_train_dense), y_train)\n",
    "# print(\"GaussianNB accuracy, on validation data: \", gnb_model.score((X_test_dense), y_test))\n",
    "\n",
    "\n",
    "# Bernoulli Naive Bayes model\n",
    "print(\"Training/Evaluating Bernoulli Naive Bayes model...\")\n",
    "bnb_model = BernoulliNB().fit(X_train_vectorized, y_train)\n",
    "print(\"BernoulliNB accuracy, on validation data: \", bnb_model.score(X_test_vectorized, y_test))\n",
    "\n",
    "\n",
    "# Mulitnomial Naive Bayes model\n",
    "print(\"Training/Evaluating Multinomial Naive Bayes model...\")\n",
    "mnb_model = MultinomialNB().fit(X_train_vectorized, y_train)\n",
    "print(\"MultinomialNB accuracy, on validation data: \", mnb_model.score(X_test_vectorized, y_test))\n",
    "\n",
    "\n",
    "# Linear SVC Model\n",
    "print(\"Training/Evaluating Linear SVC model...\")\n",
    "lsvc_model = LinearSVC().fit(X_train_vectorized_tfidf, y_train)\n",
    "print(\"Linear SVC accuracy, on validation data: \", lsvc_model.score(X_test_vectorized_tfidf, y_test))\n",
    "\n",
    "\n",
    "# Logistic Regression Model\n",
    "print(\"Training/Evaluating Logistic Regression model...\")\n",
    "lr_model = LogisticRegression().fit(X_train_vectorized, y_train)\n",
    "print(\"Logistic Regression accuracy, on validation data: \", lr_model.score(X_test_vectorized, y_test))\n",
    "\n",
    "\n",
    "# # svm.SVC model\n",
    "# clfrSVM = svm.SVC(kernel='linear', C=0.1)             # SVM Classifier training without tfidf\n",
    "# clfrSVM.fit(X_train_vectorized, y_train)\n",
    "# predicted_labels = clfrSVM.predict(X_test_vectorized)\n",
    "# print(\"SVM, Accuracy on validation data:\", accuracy_score(y_test, predicted_labels))\n",
    "#\n",
    "# clfrSVMtfidf = svm.SVC(kernel='linear', C=0.1)          # SVM Classifier training with tfidf\n",
    "# clfrSVMtfidf.fit(X_train_vectorized_tfidf, y_train)\n",
    "# predicted_labels_tfidf = clfrSVMtfidf.predict(X_test_vectorized_tfidf)\n",
    "# print(\"SVM, Accuracy on validation data with tfidf:\", accuracy_score(y_test, predicted_labels_tfidf))\n",
    "\n",
    "\n",
    "# Final statistics\n",
    "\n",
    "          # This is the actual testing data being read into a new variable\n",
    "test_doc = pd.read_csv('test_data.csv')     # reading in the testing data file for final testing\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(test_doc['review'], # creating a similar variable with test data\n",
    "                                                        test_doc['target'],\n",
    "                                                        test_size=.999999,\n",
    "                                                        random_state=0\n",
    "                                                        )\n",
    "\n",
    "X_test2_vect = vect.transform(X_test2)          # Test Data feature extraction\n",
    "# print('\\n\\nX_train shape: ', X_test2_vect.shape)      # Are we on the right track?\n",
    "X_test2_final = TfidfTransformer().fit_transform(X_test2_vect)      # Tfidf transformation of Test Data\n",
    "# X_testFinalDense = X_test2_final.toarray()        # Numpy vector smoothing\n",
    "\n",
    "# Elicitation of LinearSVC Stats\n",
    "weighted_prediction = lsvc_model.predict(X_test2_final)\n",
    "print(\"\\nLinearSVC Model statistics, on Test data (with tfidf)\")    # statistics for the LinearSVC model with tfidf\n",
    "print('Accuracy:', accuracy_score(y_test2, weighted_prediction))\n",
    "print('F1 score:', f1_score(y_test2, weighted_prediction,average='weighted'))\n",
    "print('Recall:', recall_score(y_test2, weighted_prediction,\n",
    "                              average='weighted'))\n",
    "print('Precision:', precision_score(y_test2, weighted_prediction,\n",
    "                                    average='weighted'))\n",
    "print('\\n clasification report:\\n', classification_report(y_test2, weighted_prediction))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(y_test2, weighted_prediction))\n",
    "\n",
    "# Elicitation of Logistic Regression Stats\n",
    "lrweighted_prediction = lr_model.predict(X_test2_final)\n",
    "print(\"\\nLogistic Regression Model statistics, on Test data (with tfidf)\")    # statistics for the LogisticRegression model with tfidf\n",
    "print('Accuracy:', accuracy_score(y_test2, lrweighted_prediction))\n",
    "print('F1 score:', f1_score(y_test2, lrweighted_prediction,average='weighted'))\n",
    "print('Recall:', recall_score(y_test2, lrweighted_prediction,\n",
    "                              average='weighted'))\n",
    "print('Precision:', precision_score(y_test2, lrweighted_prediction,\n",
    "                                    average='weighted'))\n",
    "print('\\n clasification report:\\n', classification_report(y_test2, lrweighted_prediction))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(y_test2, lrweighted_prediction))\n",
    "\n",
    "# Elicitation of Multinomial Naive Bayes Stats\n",
    "mnbweighted_prediction = mnb_model.predict(X_test2_final)\n",
    "print(\"\\nMultinomial Naive Bayes Model statistics, on Test data (with tfidf)\")    # statistics for the MultinomialNB model with tfidf\n",
    "print('Accuracy:', accuracy_score(y_test2, mnbweighted_prediction))\n",
    "print('F1 score:', f1_score(y_test2, mnbweighted_prediction,average='weighted'))\n",
    "print('Recall:', recall_score(y_test2, mnbweighted_prediction,\n",
    "                              average='weighted'))\n",
    "print('Precision:', precision_score(y_test2, mnbweighted_prediction,\n",
    "                                    average='weighted'))\n",
    "print('\\n clasification report:\\n', classification_report(y_test2, mnbweighted_prediction))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(y_test2, mnbweighted_prediction))\n",
    "\n",
    "# Elicitation of Bernoulli Naive Bayes Stats\n",
    "bnbweighted_prediction = bnb_model.predict(X_test2_final)\n",
    "print(\"\\nBernoulli Naive Bayes Model statistics, on Test data (with tfidf)\")    # statistics for the BernoulliNB model with tfidf\n",
    "print('Accuracy:', accuracy_score(y_test2, bnbweighted_prediction))\n",
    "print('F1 score:', f1_score(y_test2, bnbweighted_prediction,average='weighted'))\n",
    "print('Recall:', recall_score(y_test2, bnbweighted_prediction,\n",
    "                              average='weighted'))\n",
    "print('Precision:', precision_score(y_test2, bnbweighted_prediction,\n",
    "                                    average='weighted'))\n",
    "print('\\n clasification report:\\n', classification_report(y_test2, bnbweighted_prediction))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(y_test2, bnbweighted_prediction))\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"time to run: \", t1-t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n['the worst' 'waste of' 'not even' 'at all' 'to be' 'of the worst'\n 'the only' 'the original' 'lack of' 'it just']\n\nLargest Coefs:\n['the best' 'is great' 'must see' 'very good' 'my favorite' 'on dvd'\n 'as the' 'very well' 'an excellent' '10 10']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = lsvc_model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs:\\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
